{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C4B742AmKBd"
   },
   "source": [
    "# Polarization around climate change: \n",
    "## Is it growing as fast as the polar circle is shrinking? \n",
    "\n",
    "### The notebook\n",
    "\n",
    "This notebook is the result of a 4 way merge between our individual notebooks, somethings may looks silly (i.e. graph missing, cells not able to run, etc...) In this case please refer to the individual notebooks (under /notebooks) !\n",
    "\n",
    "DISCLAIMER: Viewing experience using Visual Studio Code might be suboptimal. We advise the reader to read this notebook using the Jupyter Notebook/Lab interface.\n",
    "\n",
    "NB: We're submitting a single notebook, but we strongly believe that for a better reviewing experience you should review each notebook independently under the following order :\n",
    "1. json-filtering.ipynb\n",
    "2. sentiment_analysis.ipynb\n",
    "3. word_embeddings.ipynb \n",
    "4. wikipedia.ipynb\n",
    "\n",
    "Let us explain the logic of this milestone 2 to make your reading experience easier.\n",
    "\n",
    "First, json-filtering lays out the preprocessing pipeline we had in place to filter out the quotes to only keep the ones under the theme of climate change, additionally we only keep quotes where the speaker was assigned with a probability > 0.9. We go from having the full Quotebank dataset to having a pickled dataframe of each year (2015-2020) of manageable size.\n",
    "\n",
    "The sentiment analysis part presents our study of the current best practices of sentiment analysis. From this study, we pick our sentiment analyzer and do some elementary data exploration on the 2017 data.\n",
    "\n",
    "Next, the word embeddings part showcases the pipeline put in place to go from quote to word embedding using Word2Vec. It then showcases our capability to visualize the data using word embeddings.\n",
    "\n",
    "Finally, the wikipedia part showcases our capabilities to extract features from Wikipedia using QIDs. Such features are gender, political assignation or age. Wikipedia data is quite messy and the heuristics used to extract these features are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgE8bfcl0Ka9"
   },
   "source": [
    "# Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwT12FlED1Tp",
    "outputId": "95f36a8b-7c1a-4e26-b7dd-0528c7fbea97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAH2UcrkEYhb",
    "outputId": "2d0af5f1-6d7a-4aac-d909-042283a851fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import bz2\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "\n",
    "## FIRST TIME? uncomment this to get started\n",
    "# if you dont have a token https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\n",
    "\"\"\"\n",
    "os.chdir('/content/drive/MyDrive/')\n",
    "!git clone https://USERNAME:TOKEN@github.com/epfl-ada/ada-2021-project-adada-sur-mon-bidet.git\n",
    "\"\"\"\n",
    "\n",
    "os.chdir('/content/drive/Shareddrives/ADA/ada-2021-project-adada-sur-mon-bidet/')\n",
    "import helpers.helpers as helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSJuFL7Jpwzr",
    "outputId": "6c57b3e1-af89-42d9-de38-ed3ea039da91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 9b2737e..c1f6241\n",
      "error: Your local changes to the following files would be overwritten by merge:\n",
      "\tbase_climate_dictionary.txt\n",
      "Please commit your changes or stash them before you merge.\n",
      "Aborting\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLF1E8U-PKrE",
    "outputId": "b10d317e-5af4-409d-ccc3-517977071c58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2015: 'Quotebank/quotes-2015.json.bz2',\n",
       " 2016: 'Quotebank/quotes-2016.json.bz2',\n",
       " 2017: 'Quotebank/quotes-2017.json.bz2',\n",
       " 2018: 'Quotebank/quotes-2018.json.bz2',\n",
       " 2019: 'Quotebank/quotes-2019.json.bz2',\n",
       " 2020: 'Quotebank/quotes-2020.json.bz2'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'Quotebank/'\n",
    "out_path  = 'output/'\n",
    "\n",
    "years = range(2020, 2014, -1)\n",
    "\n",
    "data_files = os.listdir(data_path)\n",
    "path_to_files = dict(zip(years, [data_path + f for f in data_files]))\n",
    "path_to_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6MS6e1dArAX",
    "outputId": "cbccecb6-94b9-45e3-98d7-105ebd4e0980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ['aerosol', 'agriculture', 'atmosphere', 'agriculture', 'atmosphere', 'biosphere', 'carbon', 'climate', 'climatology', 'coral']\n"
     ]
    }
   ],
   "source": [
    "#Load climate dict\n",
    "climate_dict = []\n",
    "with open('base_climate_dictionary.txt', 'r') as f:\n",
    "    climate_dict = f.read().split(\"\\n\")\n",
    "\n",
    "print(len(climate_dict), climate_dict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "TTeffFkDAveL",
    "outputId": "635383de-a4b8-498b-c416-66bd2900d983"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quoteID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-21-000455</th>\n",
       "      <td>2019 was a landmark year for fiverr as we comp...</td>\n",
       "      <td>Micha Kaufman</td>\n",
       "      <td>[Q26923564]</td>\n",
       "      <td>2020-02-21 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[www.fool.com]</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01-005419</th>\n",
       "      <td>councils and communities are passionate about ...</td>\n",
       "      <td>Linda Scott</td>\n",
       "      <td>[Q19667145, Q469184]</td>\n",
       "      <td>2020-03-01 16:30:28</td>\n",
       "      <td>45</td>\n",
       "      <td>[cowraguardian.com.au, wauchopegazette.com.au,...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01-026038</th>\n",
       "      <td>i will encourage anyone from the caloundra ele...</td>\n",
       "      <td>Mark McArdle</td>\n",
       "      <td>[Q6768772]</td>\n",
       "      <td>2020-04-01 15:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[www.sunshinecoastdaily.com.au]</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-24-028340</th>\n",
       "      <td>if you re a doctor that cares about the wellbe...</td>\n",
       "      <td>Fiona Stanley</td>\n",
       "      <td>[Q1653736]</td>\n",
       "      <td>2020-02-24 12:45:00</td>\n",
       "      <td>4</td>\n",
       "      <td>[watoday.com.au, www.theage.com.au, www.smh.co...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09-038856</th>\n",
       "      <td>march has the largest amount of acreage burned...</td>\n",
       "      <td>Michael Guy</td>\n",
       "      <td>[Q11107729]</td>\n",
       "      <td>2020-03-09 07:37:02</td>\n",
       "      <td>7</td>\n",
       "      <td>[kvia.com, abc17news.com, localnews8.com, www....</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           quotation  ... phase\n",
       "quoteID                                                               ...      \n",
       "2020-02-21-000455  2019 was a landmark year for fiverr as we comp...  ...     E\n",
       "2020-03-01-005419  councils and communities are passionate about ...  ...     E\n",
       "2020-04-01-026038  i will encourage anyone from the caloundra ele...  ...     E\n",
       "2020-02-24-028340  if you re a doctor that cares about the wellbe...  ...     E\n",
       "2020-03-09-038856  march has the largest amount of acreage burned...  ...     E\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"output/df\")\n",
    "_df = df.sample(n=2000)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htzulvKyzZrJ"
   },
   "source": [
    "# Preprocessing and Data Filtering\n",
    "\n",
    "Preprocessing steps are described here, but for extensive details and code please refer to notebooks/preprocessing.ipynb. Also most steps where presented in milestone 2.\n",
    "\n",
    "## json -> smaller json\n",
    "We start from provided Quotebanks json files and select quotes which match the followin criterias:\n",
    " - Only quotations with a good speaker identification confidence\n",
    " - Only quotations refering to our chosen subject are kept\n",
    " - Only domain names or nothing is kept from urls\n",
    "\n",
    "This filtering filters out around 88% of the data. We are left with workable sizes\n",
    "\n",
    "## json -> pickle\n",
    "\n",
    "Adds a little panda-related preprocessing and saves as pickle files to reduce loading times.\n",
    "\n",
    "Additional preprocessing:\n",
    "*   safety drop na\n",
    "*   index using quoteid\n",
    "*   drop irrelevant columns\n",
    "*   type correctly date and phase\n",
    "*   normalize quotes to alphanumeric lowercase characters\n",
    "\n",
    "## EDA and FP filtering\n",
    "We investigate data density:\n",
    "\n",
    "*   Distribution of occurrence (expecting exponential or power-law)\n",
    "*   Distribution of quote lengths (expect exponential or power-law)\n",
    "*   Temporal distribution\n",
    "*   Topic distribution\n",
    "\n",
    "Then we check data quality. In particular further processing is hurt by false positives. So we come up with different ways to reduce, sometimes at the cost of many datapoints, their occurrence.\n",
    "\n",
    "*   Quote quality: reading random quotes!\n",
    "  * small quotes\n",
    "  * false positives on climate topic\n",
    "    * small words: \"vegas\" contains \"gas\"\n",
    "    * \"energy\" doesn't refer to electricity\n",
    "    * \"nuclear\" and \"atmosphere\" etc are off-topic\n",
    "\n",
    "## different pickles available\n",
    "\n",
    "* df, the original version, before FP filtering\n",
    "* sanitized_df, df with much less FP\n",
    "* sanitized_strict_df, df with as little FP as possible, at the cost of a lot of data points\n",
    "\n",
    "With each of these version goes a \"*prefix*_dummies\" that records word occurrence for our vocabulary for each quote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bK-O2mHGmKN"
   },
   "source": [
    "## EDA and looking for problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "s0rfK9y2ENpK",
    "outputId": "f70b6fcd-9067-4476-ea3e-67689b091457"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quoteID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-21-000455</th>\n",
       "      <td>2019 was a landmark year for fiverr as we comp...</td>\n",
       "      <td>Micha Kaufman</td>\n",
       "      <td>[Q26923564]</td>\n",
       "      <td>2020-02-21 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[www.fool.com]</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01-005419</th>\n",
       "      <td>councils and communities are passionate about ...</td>\n",
       "      <td>Linda Scott</td>\n",
       "      <td>[Q19667145, Q469184]</td>\n",
       "      <td>2020-03-01 16:30:28</td>\n",
       "      <td>45</td>\n",
       "      <td>[cowraguardian.com.au, wauchopegazette.com.au,...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01-026038</th>\n",
       "      <td>i will encourage anyone from the caloundra ele...</td>\n",
       "      <td>Mark McArdle</td>\n",
       "      <td>[Q6768772]</td>\n",
       "      <td>2020-04-01 15:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[www.sunshinecoastdaily.com.au]</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-24-028340</th>\n",
       "      <td>if you re a doctor that cares about the wellbe...</td>\n",
       "      <td>Fiona Stanley</td>\n",
       "      <td>[Q1653736]</td>\n",
       "      <td>2020-02-24 12:45:00</td>\n",
       "      <td>4</td>\n",
       "      <td>[watoday.com.au, www.theage.com.au, www.smh.co...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09-038856</th>\n",
       "      <td>march has the largest amount of acreage burned...</td>\n",
       "      <td>Michael Guy</td>\n",
       "      <td>[Q11107729]</td>\n",
       "      <td>2020-03-09 07:37:02</td>\n",
       "      <td>7</td>\n",
       "      <td>[kvia.com, abc17news.com, localnews8.com, www....</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           quotation  ... phase\n",
       "quoteID                                                               ...      \n",
       "2020-02-21-000455  2019 was a landmark year for fiverr as we comp...  ...     E\n",
       "2020-03-01-005419  councils and communities are passionate about ...  ...     E\n",
       "2020-04-01-026038  i will encourage anyone from the caloundra ele...  ...     E\n",
       "2020-02-24-028340  if you re a doctor that cares about the wellbe...  ...     E\n",
       "2020-03-09-038856  march has the largest amount of acreage burned...  ...     E\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"output/df\")\n",
    "_df = df.sample(n=2000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuE3zLU-Fwmj"
   },
   "source": [
    "### Occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG6S5jGK8LaS"
   },
   "source": [
    "We check the distribution of quote occurences. We expect some kind of power law where most quotes are cites a little amount of times but some are very popular and reach high occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgYCmNjp39sd"
   },
   "outputs": [],
   "source": [
    "display(df[[\"numOccurrences\"]].describe())\n",
    "plt.hist(_df[\"numOccurrences\"],bins=10,log=True)\n",
    "plt.title('Histogram of climate quote occurrences (cumulative)')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('occurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3R3glB88gDt"
   },
   "outputs": [],
   "source": [
    "# most popular climate quote\n",
    "df.nlargest(10, \"numOccurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgNnybR3sLJ2"
   },
   "source": [
    "### Quote lengths\n",
    "Expecting an exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAlgXfvVGnfQ"
   },
   "outputs": [],
   "source": [
    "df[\"quoteLength\"] = df[\"quotation\"].apply( len )\n",
    "df[\"quoteWC\"] = df[\"quotation\"].apply( lambda q : len(q.split()))\n",
    "\n",
    "_df = df.sample(n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbIYGYYmWfDb"
   },
   "outputs": [],
   "source": [
    "def hist_lengths(x):\n",
    "  fig, ax = plt.subplots(2, figsize=(12, 8))\n",
    "  ax[0].hist(x[\"quoteLength\"], log = True, bins = 30)\n",
    "  ax[0].set_title(\"histogram of characters in quotes\")\n",
    "  ax[1].hist(x[\"quoteWC\"], log = True, bins = 30)\n",
    "  ax[1].set_title(\"histogram of words in quotes\")\n",
    "  return\n",
    "\n",
    "hist_lengths(_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSHebS7RYurv"
   },
   "source": [
    "#### small quotes\n",
    "\n",
    "We want to make sure even small quotes have meaning.\n",
    "Lets check a few of them to see if we have irrelevant quotes like \"Climate climate climate\" or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "3xsqUfc5XPcB",
    "outputId": "4cd64b9c-8edd-4299-eb01-f2d7f2d73c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quoteID\n",
       "2015-07-27-082359        the seductiveness of cheap gasoline \n",
       "2015-06-25-026765                              i ll cop that \n",
       "2016-09-27-064724                   it was not the coalition \n",
       "2018-09-17-021329                   focus even more energy on\n",
       "2019-03-27-092439                  the climate is so extreme \n",
       "2018-05-10-004043           advancing clean energy solutions \n",
       "2015-08-10-009263                  because it s unrefined gas\n",
       "2019-04-30-023522                    fuel for new innovation \n",
       "2015-03-31-102395               world s worst climate villain\n",
       "2017-06-15-129244    this represents value added agriculture \n",
       "Name: quotation, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "quoteID\n",
       "2020-02-18-070635            strong sustainable concept \n",
       "2020-02-26-058255                       size scope scale\n",
       "2020-01-07-080448                warming drying climate \n",
       "2019-01-15-047739          inclusive sustainable growth \n",
       "2019-04-16-041825    irrepressible irresponsible energy \n",
       "2019-10-29-012869                  calm cerebral energy \n",
       "2019-04-05-023701                         game on vegas \n",
       "2019-10-24-000108                        climate change \n",
       "2019-10-31-086128        stable sustainable development \n",
       "2019-03-10-037534                       the gas chamber \n",
       "2019-07-08-002705                         all his energy\n",
       "2019-10-20-010429              genuine warming charisma \n",
       "2019-05-03-081411                     still dodgin cops \n",
       "2019-10-30-000115                          dont copy me \n",
       "2018-04-29-017901         holistic biodynamic ecosystem \n",
       "2018-05-18-085088                     putting his energy\n",
       "2018-01-08-022335                      fresh new energy \n",
       "2018-07-20-058650                      organ orgasm orgy\n",
       "2018-02-06-079290          microscopic underwater world \n",
       "2018-11-21-136509                     you say cacophony \n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def small_quote(df, threshold = 5):\n",
    "  df[\"quoteWC\"] = df[\"quotation\"].apply( lambda q : len(q.split()))\n",
    "  return df.query(\"quoteWC <=\" + str(threshold))\n",
    "\n",
    "display(small_quote(_df)[\"quotation\"].head(10))\n",
    "small_quote(df, threshold=5)[\"quoteWC\"].value_counts()\n",
    "\n",
    "small_quote(df, threshold=3)[\"quotation\"][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hg5_ZWIWP5ng"
   },
   "source": [
    "It seems ok. Do not need more preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1O0CcGfO5e6"
   },
   "source": [
    "### Time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtbf9FXLPDEZ"
   },
   "outputs": [],
   "source": [
    "datetime_index = df.reset_index().quoteID.apply(lambda x: x[:10])\n",
    "date_df = df.set_index(datetime_index)\n",
    "date_df.index = pd.to_datetime(date_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLhcjGxJzAU6"
   },
   "outputs": [],
   "source": [
    "occs = date_df.groupby(pd.Grouper(freq=\"3M\"))[\"numOccurrences\"].aggregate([\"sum\", \"count\"])\n",
    "sns.lineplot(data=occs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0NMyYsWw_BT"
   },
   "source": [
    "### Topic distribution\n",
    "\n",
    "We would like to see what sub-topics on climate change are prominent. If they are evenly represented etc. For example if 50% of quotes are about nuclear power, we should keep it in mind in further conclusions.\n",
    "\n",
    "For further use we generate a dataframe with dummies that can easily be attached if needed. This df is relatively wide and quite sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7PFmRwHw4Dm"
   },
   "outputs": [],
   "source": [
    "# generate dummies\n",
    "dm = df[[\"quotation\"]].copy()\n",
    "\n",
    "for w in climate_dict:\n",
    "  dm[w] = dm[\"quotation\"].apply(lambda q : 1 if w in q else 0)\n",
    "\n",
    "# plot occurrence and top 5\n",
    "x = dm.describe().loc[\"mean\"]\n",
    "fig = plt.subplot()\n",
    "fig.hist(x.values)\n",
    "fig.set_title(\"mean occurrence of words in a quotes\")\n",
    "fig.set_ylabel(\"words count\")\n",
    "fig.set_xlabel(\"occurrence probability of word in a quote\")\n",
    "\n",
    "display(x.nlargest(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfXeRaGRxjoC"
   },
   "source": [
    "As expected from previous analysis, energy quotes are very common, but most are filtered out in better versions of our preprocessed data. Also some vocabulary words are very rare, but that's expected for example for \"ar4\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl7kx4K1w4Dm"
   },
   "outputs": [],
   "source": [
    "def generate_dummies(df, vocab, out=None):\n",
    "  dm = df[[\"quotation\"]].copy()\n",
    "\n",
    "  for w in vocab:\n",
    "    dm[w] = dm[\"quotation\"].apply(lambda q : 1 if w in q else 0)\n",
    "  \n",
    "  if(out):\n",
    "    dm.to_pickle(out)\n",
    "  return dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "245ZBTNjDMmc"
   },
   "source": [
    "### Climate topic false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZQiQRZ5D-VY"
   },
   "source": [
    "False positives might be a problem if there are too many, as further conclusions might be biased. First, we browse through at least a 100 quotes to identify false positive reccuring examples and a idea of the FP rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDqT0xo0y4zC"
   },
   "source": [
    "TOTAL read : 100\n",
    "of which FP: 20\n",
    "\n",
    "Which is on the high-side.\n",
    "\n",
    "In the following subsections, we replay some FP recurring examples and deal with them to reduce this FP rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5_REunNmANR"
   },
   "source": [
    "#### Small words\n",
    "\n",
    "Example:\n",
    "\n",
    "\"we re going to hop on a plane head to new york and show people what ve**gas** is all about\"\n",
    "\n",
    "==> small words can end up randomly in the middle of other words\n",
    "\n",
    "Instead of looking for dictionnary words in the quotation as a list of chars, we can look for words in the list of words. The disadvantage is that it will result in excluding all words from the same family that don't exactly match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkDonxfWl-BL",
    "outputId": "1aa7af46-35b8-47bc-a4e8-fcbd36cdfef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact words elimination ratio :  0.25\n",
      "small words elimination ratio :  0.0805\n"
     ]
    }
   ],
   "source": [
    " _df = df.sample(n=4000) #work on a random subset\n",
    "\n",
    "# the number of quotes which have an exact word from the dictionnary, rather the substring\n",
    "exact_words = sum([1 if any([w in q.split() for w in climate_dict]) else 0 for q in _df[\"quotation\"]])\n",
    "\n",
    "# the number of quotes in which there is a small word as a substring but not as a word\n",
    "def small_words_fp(quote, vocab, small_thresh = 5):\n",
    "  small_words = [w for w in vocab if len(w) < small_thresh]\n",
    "  in_string = any([w in quote for w in small_words])\n",
    "  in_words  = any([w in quote.split() for w in small_words])\n",
    "  return  in_string and not in_words\n",
    "\n",
    "small_fp = sum(_df[\"quotation\"].apply(lambda x : small_words_fp(x, climate_dict)))\n",
    "\n",
    "#Compare the elimination ratio if we are to preprocess using each method\n",
    "print(\"exact words elimination ratio : \", (4000 - exact_words)/ 4000)\n",
    "print(\"small words elimination ratio : \", (small_fp)/ 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bm4CwaJZrvaZ"
   },
   "source": [
    "Depending on seed, the very conservative *exact words only* results in 15% loss which is a bit more then we are comfortable with. Considering only small words for this sanitazation we have about 8% of loss which is much more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_4dCf6jrumU",
    "outputId": "a0d08bf9-6840-4214-b629-d696fb2abe88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379625"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to remove quotes with small words as substring but not as \n",
    "def sanitize_small_word_fp(df):\n",
    "  to_drop = df.loc[df[\"quotation\"].apply(lambda q : small_words_fp(q, climate_dict))].index\n",
    "  return df.drop(index=to_drop)\n",
    "\n",
    "sanitize_small_word_fp(df).index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0yPyy3yyI-g"
   },
   "source": [
    "#### Energy\n",
    "\n",
    "Examples:\n",
    "\n",
    "\"players still have to get the same kind of visceral energy that they d get if they had a real audience \"\n",
    "\n",
    "==> Energy has a variety of meanings which don't correspond to our down to earth physical meaning, and thus results in a lot of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRtr77_YzkjL"
   },
   "outputs": [],
   "source": [
    "#look in more detail at energy quotes\n",
    "def energy_only(q, vocab = climate_dict):\n",
    "  energy = \"energy\" in q\n",
    "  others = any([word in q for word in vocab if word != \"energy\"])\n",
    "  return energy and not others\n",
    "\n",
    "energy_only_quotes = df.loc[df[\"quotation\"].apply(lambda q : energy_only(q))]\n",
    "\n",
    "print(\"energy only quotes: \", energy_only_quotes.index.size, \"  ratio : \", energy_only_quotes.index.size / df.index.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teLyQGbPzkCe"
   },
   "source": [
    "<img src=\"https://i.imgflip.com/5vx8at.jpg\" title=\"made at imgflip.com\"/>\n",
    "\n",
    "Unfortunately the share of quotes containing only energy is consequent, and we must find another way to isolate the false positives. Around half are FP. \n",
    "\n",
    "Most recurring false positives are about sports, shows, and trait of character. The first two can be isolated rather easily, but the last is much harder. We try to either isolate our meaning of energy, or isolate the wrong meanings of energy by searching with more specific queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9QPr4K65wBD",
    "outputId": "edbe44d1-b5cd-4564-badb-2ac25417f92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7861 0.11841530466219778\n",
      "21261 0.32026813286133915\n",
      "overlap :  6948\n"
     ]
    }
   ],
   "source": [
    "## Refined energy queries\n",
    "better_energy = [\"wind energy\", \"solar energy\", \"hydro energy\", \"clean energy\",\n",
    "                 \"energy policy\", \"energy compan\", \"geothermal energy\",\n",
    "                 \"energy sector\",\"energy storage\", \"renewable energy\", \"energy consumption\"]\n",
    "\n",
    "better_energy_quotes = energy_only_quotes.loc[energy_only_quotes[\"quotation\"].apply(lambda q : any([w in q for w in better_energy]))]\n",
    "\n",
    "## Isolating sport and shows energy references\n",
    "abstract_energy = \"league stadium show star play sport song team coach player game audience kid actor actress\"\n",
    "abstract_energy = abstract_energy + \" boy girl olympic fans supporters health healthy nutrients\"\n",
    "abstract_energy = abstract_energy.split()\n",
    "abstract_energy.append(\"my energy\")\n",
    "\n",
    "abstract_energy_quotes = energy_only_quotes.loc[energy_only_quotes[\"quotation\"].apply(lambda q : any([w in q for w in abstract_energy]))]\n",
    "\n",
    "print(\"better energy queries rate amongst energy only quotes: \" ,\n",
    "      better_energy_quotes.index.size / energy_only_quotes.index.size)\n",
    "print(\"sport and shows energy quote rate amongst energy quotes : \",\n",
    "      abstract_energy_quotes.index.size / energy_only_quotes.index.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqMwAgNeGS-n"
   },
   "source": [
    "Trying to isolating wrong meanings only result in ~30% of our set, but we know we have around 50%, so it leaves a lot of FP.\n",
    "\n",
    "Being more conservative and keeping only the refined energy queries, we retain 11%. This means loosing on quite a lot of TP, but removing all energy FP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUCEsaCwAsO9",
    "outputId": "b15e7a18-8786-4961-e6a9-0113e8bf9afb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.142392081867418"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize_energy_conservatory(df):\n",
    "\n",
    "  def energy_only(q, vocab = climate_dict):\n",
    "    energy = \"energy\" in q\n",
    "    others = any([word in q for word in vocab if word != \"energy\"])\n",
    "    return energy and not others\n",
    "\n",
    "  energy_only_quotes = df.loc[df[\"quotation\"].apply(lambda q : energy_only(q))]\n",
    "  better_energy = [\"wind energy\", \"solar energy\", \"hydro energy\", \"clean energy\",\n",
    "                   \"energy policy\", \"energy compan\", \"geothermal energy\", \"energy sector\",\n",
    "                   \"energy storage\", \"renewable energy\", \"energy consumption\"]\n",
    "  better_energy_quotes = energy_only_quotes.loc[energy_only_quotes[\"quotation\"].apply(lambda q : any([w in q for w in better_energy]))]\n",
    "\n",
    "  to_drop = set(energy_only_quotes.index)-set(better_energy_quotes.index)\n",
    "  return df.drop(to_drop)\n",
    "\n",
    "\n",
    "(df.index.size - sanitize_energy_conservatory(df).index.size) / df.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Me4K--B1DdvZ",
    "outputId": "2666d18b-f090-4c78-9a2b-8633c2133451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05172917183690749"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sanitize_energy_permissive(df):\n",
    "\n",
    "  def energy_only(q, vocab = climate_dict):\n",
    "    energy = \"energy\" in q\n",
    "    others = any([word in q for word in vocab if word != \"energy\"])\n",
    "    return energy and not others\n",
    "\n",
    "  energy_only_quotes = df.loc[df[\"quotation\"].apply(lambda q : energy_only(q))]\n",
    "\n",
    "  abstract_energy = \"league stadium show star play sport song team coach player \" + \\\n",
    "                    \"game audience kid actor actress boy girl olympic fans supporters health healthy nutrients\"\n",
    "  abstract_energy = abstract_energy.split()\n",
    "  abstract_energy.append(\"my energy\")\n",
    "  abstract_energy_quotes = energy_only_quotes.loc[energy_only_quotes[\"quotation\"].apply(lambda q : any([w in q for w in abstract_energy]))]\n",
    "\n",
    "  return df.drop(abstract_energy_quotes.index)\n",
    "\n",
    "(df.index.size - sanitize_energy_permissive(df).index.size) / df.index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gu936561-5CW"
   },
   "source": [
    "#### strict removal of FP-prone vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssJM2M5i_BCv",
    "outputId": "8932cb58-c9dd-4369-9156-45ea55235cf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2066076894254585"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def w_only(q, w, vocab = climate_dict):\n",
    "  w_in_quote = w in q\n",
    "  any_other_in_quote = any([word in q for word in vocab if word != w])\n",
    "  return w_in_quote and not any_other_in_quote\n",
    "\n",
    "FPprone = [\"cop\", \"atmosphere\", \"nuclear\", \"ecosystem\"]\n",
    "\n",
    "def sanitize_fpprone_vocab(df, words):\n",
    "  to_remove = pd.Index([])\n",
    "  for w in words:\n",
    "    w_only_idx = df.loc[df[\"quotation\"].apply(lambda q : w_only(q, w))].index\n",
    "    to_remove = to_remove.append(w_only_idx)\n",
    "  \n",
    "  return df.drop(to_remove)\n",
    "\n",
    "(df.index.size - sanitize_fpprone_vocab(df, FPprone).index.size) / df.index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqEIrFGtQtEa"
   },
   "source": [
    "## Sanitized df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy4MYQ5HJM8Z"
   },
   "outputs": [],
   "source": [
    "# dummies for df\n",
    "generate_dummies(df, climate_dict, out=\"output/df_dummies\");\n",
    "\n",
    "# less FP and its dummies\n",
    "sanitized_df = sanitize_energy_conservatory(sanitize_small_word_fp(df))\n",
    "sanitized_df.to_pickle(\"output/sanitized_df\")\n",
    "generate_dummies(sanitized_df, climate_dict, out=\"output/sanitized_dummies\");\n",
    "print(\"sanitized df keep ratio: \", sanitized_df.index.size / df.index.size))\n",
    "\n",
    "# even less FP and its dummies\n",
    "FPprone = [\"cop\", \"atmosphere\", \"nuclear\", \"ecosystem\"]\n",
    "sanitized_strict_df = sanitize_fpprone_vocab(sanitized_df, FPprone)\n",
    "sanitized_strict_df.to_pickle(\"output/sanitized_strict_df\")\n",
    "generate_dummies(sanitized_strict_df, climate_dict, out=\"output/sanitized_strict_dummies\");\n",
    "print(\"sanitized strict df keep ratio: \", (sanitized_strict_df.index.size / df.index.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT6AzWhWLjW1"
   },
   "source": [
    "# Who talks about Climate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXZrR64FLonM"
   },
   "source": [
    "## Most famous quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z94SLsS-Liw6",
    "outputId": "254e46b4-1a86-42dc-92ca-df19b5bb2ba4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i lost so i m going to follow our democratic traditions poison the wells and scorch the earth',\n",
       "       'this is not an opportunity to go outside and try to have fun with a hurricane ',\n",
       "       'this is a precautionary measure to ensure we have enough fuel to support lifesaving efforts respond to the storm and restore critical services and critical infrastructure ',\n",
       "       'this is when the taiwanese people show their calm resilience and love ',\n",
       "       'pretend assume presume that a major hurricane is going to hit right smack dab in the middle of south carolina and is going to go way inshore '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sanitized_strict_df\n",
    "\n",
    "top5 = data.nlargest(n= 5, columns=\"numOccurrences\")[[\"quotation\", \"speaker\", \"numOccurrences\"]]\n",
    "top5[\"quotation\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW_e8OzJM8_W"
   },
   "source": [
    "## Most famous speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "EQizYZDMM7DD",
    "outputId": "c03eaa42-0f5c-43f9-a9a0-3bf4c40e4418"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Narendra Modi</th>\n",
       "      <td>3566</td>\n",
       "      <td>Narendra Modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Josh Frydenberg</th>\n",
       "      <td>3131</td>\n",
       "      <td>Josh Frydenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antonio Guterres</th>\n",
       "      <td>2931</td>\n",
       "      <td>Antonio Guterres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scott Morrison</th>\n",
       "      <td>2604</td>\n",
       "      <td>Scott Morrison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malcolm Turnbull</th>\n",
       "      <td>2433</td>\n",
       "      <td>Malcolm Turnbull</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  numOccurrences           speaker\n",
       "speaker                                           \n",
       "Narendra Modi               3566     Narendra Modi\n",
       "Josh Frydenberg             3131   Josh Frydenberg\n",
       "Antonio Guterres            2931  Antonio Guterres\n",
       "Scott Morrison              2604    Scott Morrison\n",
       "Malcolm Turnbull            2433  Malcolm Turnbull"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sanitized_strict_df\n",
    "\n",
    "speakers = data[[\"speaker\", \"numOccurrences\"]].groupby(\"speaker\").sum()\n",
    "speakers[\"speaker\"] = speakers.index\n",
    "speakers.nlargest(n=5, columns=\"numOccurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX7WBbgzqzvI"
   },
   "outputs": [],
   "source": [
    "speakers[[\"famous_quote\",\"famous_quote_occs\"]] = data.groupby(\"speaker\")[[\"quotation\", \"numOccurrences\"]].max()\n",
    "speakers.nlargest(n=5, columns=\"numOccurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xnv0_4utqba",
    "outputId": "f6029828-18dd-4141-ff38-16dcc595a97d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72441"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQuBJh9Et05p"
   },
   "outputs": [],
   "source": [
    "fig = sns.scatterplot(data=speakers, x=\"numOccurrences\", y=\"famous_quote_occs\", )\n",
    "fig.set(xlabel=\"Cumulated quotations\", ylabel='Occurrence of best quote')\n",
    "fig.set_title(\"speaker quote occurrence relationship\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNeiY5p34ai_"
   },
   "source": [
    "## WRT time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-EbgFuL4Zv_"
   },
   "outputs": [],
   "source": [
    "datetime_index = data.reset_index().quoteID.apply(lambda x: x[:10])\n",
    "date_df = data.set_index(datetime_index)\n",
    "date_df.index = pd.to_datetime(date_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOhNLZro4xv6"
   },
   "outputs": [],
   "source": [
    "def top_speakers(data, n=5):\n",
    "  s = data[[\"speaker\", \"numOccurrences\"]].groupby(\"speaker\").sum()\n",
    "  return s.nlargest(n, columns=\"numOccurrences\")\n",
    "\n",
    "grouped_month = date_df.groupby(pd.Grouper(freq=\"1M\"))[[\"speaker\", \"numOccurrences\"]]\n",
    "\n",
    "acc = []\n",
    "for month, group in grouped_month:\n",
    "  x = top_speakers(group, n=1000).T\n",
    "  x.index = [month]\n",
    "  acc.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpmLBLMo9pR3"
   },
   "outputs": [],
   "source": [
    "monthly_speakers = pd.concat(acc, join=\"outer\").fillna(0)\n",
    "cum_speakers = monthly_speakers.cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wfGJLUrAvF-"
   },
   "outputs": [],
   "source": [
    "import bar_chart_race as bcr\n",
    "bcr.bar_chart_race(cum_speakers, n_bars=6, steps_per_period=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p-t3igPmKB2"
   },
   "source": [
    "# Review of sentiment analysis\n",
    "\n",
    "The sentiment analysis part presents our study of the current best practices of sentiment analysis. From this study, we pick our sentiment analyzer and do some elementary data exploration on the 2017 data.\n",
    "\n",
    "\n",
    "### Choice of sentiment analyzer\n",
    "\n",
    "The classic gold standard lexicon, especially for longer text, is LIWC (Linguistic Inquiry and Word Count) [[1]]. It is a Semantic Orientation (Polarity-based) Lexicon. Sociologists, psychologists, linguists, and computer scientists find LIWC appealing because it has been extensively validated. Also, its straightforward dictionary and simple word lists are easily inspected, understood, and extended if desired. Such attributes make LIWC an attractive option to researchers looking for a reliable lexicon to extract emotional or sentiment polarity from text. \n",
    "\n",
    "But LIWC is unable to account for differences in the sentiment intensity of words. For example, “The food here is exceptional” conveys more positive intensity than “The food here is okay”. A sentiment analysis tool using LIWC would score them equally (they each contain one positive term). Such distinctions are intuitively valuable for fine-grained sentiment analysis and in our case to detect polarization between two opinions on climate. \"I am skeptic about climate\" is not as intense as \"I hate Greta Thunberg\" and we should be able to detect it.\n",
    "\n",
    "Another aspect to take into account is that a given sentiment analyzer performs differenlty depending on the length of quotes. In our dataset, we have a mixture of short quotes and long quotes with a majority of shorter quotes.\n",
    "\n",
    "Ease of use such as the need (or not) to train the sentiment analyzer has to be taken into account.\n",
    "\n",
    "Given all these factors, we have decided  use VADER [[2]] (Valence Aware Dictionary and sEntiment Reasoner). It is pretrained and built into NLTK.\n",
    "\n",
    "Reading the paper, we know that VADER is best suited for language used in social media and short text. \n",
    "\n",
    "VADER is the result of very thorough work. It has been trained on its own valence-aware sentiment lexicon composed of other well established/ \"gold standard\" sentiment banks such as ANEW (Affective Norms for English Words) [[4]] for sentiment valence ranging from [1-9], LIWC mentioned before and the Genereal Inquirer (GI) [[3]]. On top of that, it incorporates numerous lexical features common to sentiment expression in microblogs.\n",
    "\n",
    "In the paper, it was shown that VADER (F1 = 0.96) outperforms individual human raters (F1 = 0.84) at correctly classifying the sentiment of tweets into positive, neutral, or negative classes. Furthermore, it was shown to generalize very well and to outperform other analyzers outside of social media text and longer text.\n",
    "\n",
    "\n",
    "We also went through the ADA lectures on text analysis and remarked that VADER was also used, further convincing us that it is indeed a quality choice.\n",
    "\n",
    "\n",
    "### Scoring:\n",
    "\n",
    "Given a sentence, we can use VADER to compute polarity_scores() which will ouptput a dictionary of 4 values ('compound', 'neg', 'neu', 'pos').\n",
    "\n",
    " The 'compound' score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of polarity for a given sentence.\n",
    "\n",
    "The pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all add up to be 1 or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence.\n",
    "\n",
    "\n",
    "\n",
    "[2]: http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\n",
    "\n",
    "[3]: http://www.wjh.harvard.edu/~inquirer/\n",
    "\n",
    "[1]: https://liwc.wpengine.com/\n",
    "\n",
    "[4]: https://csea.phhp.ufl.edu/media/anewmessage.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEykOnClmKB7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6louhwYmKB9"
   },
   "outputs": [],
   "source": [
    "## Setting up a sample dataframe\n",
    "\n",
    "df = pd.read_pickle(\"df2017_0\")\n",
    "df['compound'] = df.quotation.apply(lambda x : sia.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJcktnRamKB9"
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Given text, outputs VADER polarity scores with explanations\n",
    "\n",
    "    Args:\n",
    "        text (string)\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    polarity_scores = sia.polarity_scores(text)\n",
    "    print(f\"Portion of the text which is negative: {polarity_scores['neg']}.\")\n",
    "    print(f\"Portion of the text which is neutral: {polarity_scores['neu']}.\")\n",
    "    print(f\"Portion of the text which is positive: {polarity_scores['pos']}.\")\n",
    "\n",
    "    print(f\"Normalized weighted average valence score of the text: {polarity_scores['compound']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34gy0Cg3mKB9"
   },
   "outputs": [],
   "source": [
    "def plot_sentiment_hist(df,sentiment='compound',all=False):\n",
    "    \"\"\"\"\n",
    "    Given the dataframe with quotations, plots the distribution of a given component of polarity_scores() or all components.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe with quotations \n",
    "        sentiment (str, optional): Can be 'pos' , 'neg', 'neu' or 'compound'. Defaults to 'compound'. Is ignored if all=True\n",
    "        all (bool, optional): if true, plot the distribution of all components. Defaults to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    if all:\n",
    "        f,a = plt.subplots(2,2,figsize=(15,7),sharey=True)\n",
    "        sentiments = ['neg','neu','pos','compound']\n",
    "        transformed = df.quotation.apply(lambda x : sia.polarity_scores(x))\n",
    "        f.suptitle(\"Distribution of 'neg', 'neu', 'pos' and 'compound' in the given corpus\")\n",
    "\n",
    "        for i, sent in enumerate(sentiments):\n",
    "            idx = divmod(i,2)\n",
    "            g = sns.histplot(data=transformed.apply(lambda x: x[sent]), bins='auto',ax=a[idx[0],idx[1]])\n",
    "            g.set_xlabel(f\"{sent} score\")\n",
    "            g.set_yscale('log')\n",
    "    else:\n",
    "\n",
    "        transformed = df.quotation.apply(lambda x : sia.polarity_scores(x)[sentiment])\n",
    "        f, a = plt.subplots(figsize=(15, 5))\n",
    "        f.suptitle(f\"Distribution of {sentiment} sentiment\")\n",
    "        g= sns.histplot(data=test,bins='auto')\n",
    "        g.set_xlabel(f\"{sentiment} score\")\n",
    "        g.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jv3Ws3gmKB9"
   },
   "outputs": [],
   "source": [
    "def plot_compound_time_series(df, freq = \"W\"):\n",
    "    \"\"\"\n",
    "    Given a dataframe assumed to have a \"compound\" column, plots the time series of mutliple aggregates of compound at given frequency (W = week, M = month)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe asssumed to have a \"compound\" column\n",
    "        freq (str, optional): The frequency of our time series i.e. at frequency do we take our aggregates. Defaults to \"W\".\n",
    "    \"\"\"\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    ## changing the index into datetime\n",
    "\n",
    "    new_index = df.reset_index().quoteID.apply(lambda x: x[:10])\n",
    "    new_df = df.set_index(new_index)\n",
    "    new_df.index = pd.to_datetime(new_df.index)\n",
    "    \n",
    "    mean_compound_values_time_series = new_df.compound.groupby(pd.Grouper(freq=freq)).mean()\n",
    "    sd_compound_values_time_series = new_df.compound.groupby(pd.Grouper(freq=freq)).std()\n",
    "    x = range(0,len(mean_compound_values_time_series))\n",
    "\n",
    "    f, a = plt.subplots(1,2,figsize=(15, 5))\n",
    "   \n",
    "    g= sns.lineplot(x=x, y=mean_compound_values_time_series, ax = a[0])\n",
    "    g.set_title(f\"The time series of the mean value 'compound' at frequency {freq}\")\n",
    "    g.set_xlabel(f\"Time steps at frequency {freq}\")\n",
    "    g.set_ylabel(f\"Mean of compound at frequency {freq}\")\n",
    "\n",
    "\n",
    "    g= sns.lineplot(x=x, y=sd_compound_values_time_series, ax = a[1])\n",
    "    g.set_title(f\"The time series of the standard deviation of the value 'compound' at frequency {freq}\")\n",
    "    g.set_xlabel(f\"Time steps at frequency {freq}\")\n",
    "    g.set_ylabel(f\"Mean of compound at frequency {freq}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "it31ToDymKB9"
   },
   "outputs": [],
   "source": [
    "plot_compound_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6rEZlcNmKB9"
   },
   "source": [
    "We can see here that the weekly mean compound score oscillates around 0.3 (meaning slightly positive) but plotting the standard deviation, we can see that there are huge variations around this mean. This is the indicator of our more \"polarized\" quotes. Investigation into quantizing the compound scores into bins e.g. ([-1, -0.5], [-0.5, 0], [0, 0.5], [0.5, 1] or finer quantization) and looking at most frequent values in these bins should prove to be insightful and let us capture insights about polarization more easily than aggregates.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw3f8nO2mKB9"
   },
   "outputs": [],
   "source": [
    "plot_sentiment_hist(df,all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6AImhMOmKB9"
   },
   "source": [
    "In truly polarized data, we would expect to see a bimodal distribution in the 'compound' distribution. The somewhat uniform distribution of compound might come from two possibilites:\n",
    "\n",
    "1. The data isn't yet filtered well enough to only capture quotes about climate change\n",
    "2. Data about climate change isn't polarized enough for us to see a bimodal distribution\n",
    "\n",
    "Nonetheless, we can see slighlty more mass on the positive side, which is an indicator of more positive quotes in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rykrJS0MmKB-"
   },
   "source": [
    "# Word Embedding\n",
    "\n",
    "The word embedding part showcases the pipeline put in place to go from quote to word embedding using Word2Vec. It then showcases our capability to visualize the data using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxgHZjVJmKB-",
    "outputId": "a85d7fc9-68e0-4ca3-817f-52ba4bc72492"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\antom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\antom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./helpers/')\n",
    "from helpers import get_samples\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import visual as viz\n",
    "import w2v as w2v\n",
    "import text_tools as tt\n",
    "reload(helpers)\n",
    "reload(tt)\n",
    "reload(w2v)\n",
    "reload(viz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_u7IxZmbmKB-"
   },
   "outputs": [],
   "source": [
    "df = get_samples(num_samples=5000, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCQzlZ5KmKB-"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zonthMHumKB-"
   },
   "outputs": [],
   "source": [
    "#### Average length of the text before any preprocess : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJU_XwB3mKB-"
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"qids\", \"probas\", \"phase\", \"quoteID\", \"urls\"], axis=1)\n",
    "#sns.histplot(data=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZQl8iVKmKB-"
   },
   "outputs": [],
   "source": [
    "df[\"quote_len\"] = df.quotation.apply(tt.get_tokens).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjnJYXg9mKB-"
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots(figsize=(15, 5))\n",
    "sns.histplot(data=df, x=\"quote_len\", kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkczKYJ-mKB-",
    "outputId": "e1431fd4-fa42-4853-cac0-22be121ee853"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_low</th>\n",
       "      <th>mean_computed</th>\n",
       "      <th>mean_high</th>\n",
       "      <th>std_low</th>\n",
       "      <th>std_computed</th>\n",
       "      <th>std_high</th>\n",
       "      <th>&lt;lambda&gt;_low</th>\n",
       "      <th>&lt;lambda&gt;_computed</th>\n",
       "      <th>&lt;lambda&gt;_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quote_len</th>\n",
       "      <td>27.830818</td>\n",
       "      <td>28.391003</td>\n",
       "      <td>29.121095</td>\n",
       "      <td>21.188133</td>\n",
       "      <td>23.612752</td>\n",
       "      <td>26.448992</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean_low  mean_computed  mean_high    std_low  std_computed  \\\n",
       "quote_len  27.830818      28.391003  29.121095  21.188133     23.612752   \n",
       "\n",
       "            std_high  <lambda>_low  <lambda>_computed  <lambda>_high  \n",
       "quote_len  26.448992           5.0                5.0            5.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.CIs(data=df, columns=[\"quote_len\"], funcs=[np.mean, np.std, lambda x : np.percentile(x, 0.5)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBTQ94ibmKB-",
    "outputId": "0be59c11-5fb5-482e-f843-8f04afea0fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took : 7.52 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "\n",
    "df[\"prep_quote\"] = df.quotation.apply(tt.preprocess_quote)\n",
    "print(f\"It took : {round(time() - start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WgcDzm7mKB_"
   },
   "outputs": [],
   "source": [
    "df[\"prep_token_nb\"] = df.prep_quote.apply(len)\n",
    "f, a = plt.subplots(figsize=(15, 5))\n",
    "sns.histplot(data=df, x=\"prep_token_nb\", kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAMAkW1EmKB_",
    "outputId": "2d4a4cec-5036-4693-ecdc-6cfcd398b664"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_low</th>\n",
       "      <th>mean_computed</th>\n",
       "      <th>mean_high</th>\n",
       "      <th>std_low</th>\n",
       "      <th>std_computed</th>\n",
       "      <th>std_high</th>\n",
       "      <th>median_low</th>\n",
       "      <th>median_computed</th>\n",
       "      <th>median_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prep_token_nb</th>\n",
       "      <td>21.752234</td>\n",
       "      <td>22.348119</td>\n",
       "      <td>22.982704</td>\n",
       "      <td>18.231695</td>\n",
       "      <td>20.732495</td>\n",
       "      <td>23.824741</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean_low  mean_computed  mean_high    std_low  std_computed  \\\n",
       "prep_token_nb  21.752234      22.348119  22.982704  18.231695     20.732495   \n",
       "\n",
       "                std_high  median_low  median_computed  median_high  \n",
       "prep_token_nb  23.824741        17.0             17.0         17.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.CIs(data=df, columns=[\"prep_token_nb\"], funcs=[np.mean, np.std, np.median]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quL_JUwVmKB_",
    "outputId": "000dda57-ac82-4a8f-8189-46bbb2976401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 27, 2020 Letter to Cape Cod Second Homeowners: Cape Cod is home to over 214,000 year-round residents, who appreciate and depend upon our seasonal influx of visitors and second homeowners. It has been our way of life for centuries. During the coronavirus crisis, we all understand the desire to come to your second home on the Cape while sheltering in place. We are asking that if you do so, please help us all to remain safe and healthy by following these actions: Individuals traveling to Cape Cod from off-Cape and out of state are to self-quarantine for 14 days to avoid spreading the virus. Bring items that you will need during your stay, including prescriptions, groceries, cleaning supplies, personal health items and personal protective equipment. While essential service establishments may be open, there are shortages being experienced of key items. Support our restaurants with take-out orders as found on this list https://www.capecodchamber.org / restaurants/restaurants-offering-takeout / Check in with an elderly neighbor (from a safe distance of 6' or more -- preferably by phone) to see if they are well or need any supplies. Please stay in small groups and practice all the recommendations of the CDC for self-care. https://bit.ly/2QSmyUN Be aware that not all states have put into place the same restrictions related to Coronavirus (COVID-19). The Commonwealth of Massachusetts emergency actions are posted here: https://bit.ly/3bsZunl Call the Massachusetts state-supported 2-1-1 hotline. Whether you come to your second home here, or choose to remain in your primary home during this unprecedented time, please contribute to: Cape Cod Healthcare https://bit.ly/39lk2MX  Donations of both money and personal protective equipment are needed for local healthcare workers. They will be protecting you and your family, too, if you are here! To provide humanitarian aid to local individuals and families, many of whom are now without jobs, with no guarantee when those jobs will return, or who do not have health insurance, may lose housing or other benefits to support them, we recommend these organizations: Cape Cod Community Foundation Strategic Emergency Response Fund: https://www.capecodfoundation.org/community-response-to-covid-19/ Cape Cod & Islands Major Crisis Relief Fund: https://majorcrisisrelieffund.org/ Cape & Island United Way Community Response Fund: https://www.capeandislandsuw.org/ Cape Cod COVID-19 Workforce Housing Relief Fund http://www.haconcapecod.org Together, we will weather this global storm! Wishing you peace and health, Wendy K. Northcross, CEO Cape Cod Chamber of Commerce Michael K. Lauf, CEO John Yunits, County Administrator Barnstable County\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "['march', 'letter', 'cape', 'cod', 'second', 'homeowners', 'cape', 'cod', 'home', 'year', 'round', 'residents', 'appreciate', 'depend', 'upon', 'seasonal', 'influx', 'visitors', 'second', 'homeowners', 'way', 'life', 'centuries', 'coronavirus', 'crisis', 'understand', 'desire', 'second', 'home', 'cape', 'sheltering', 'place', 'asking', 'please', 'help', 'us', 'remain', 'safe', 'healthy', 'following', 'actions', 'individuals', 'traveling', 'cape', 'cod', 'cape', 'state', 'self', 'quarantine', 'days', 'avoid', 'spreading', 'virus', 'bring', 'items', 'need', 'stay', 'including', 'prescriptions', 'groceries', 'cleaning', 'supplies', 'personal', 'health', 'items', 'personal', 'protective', 'equipment', 'essential', 'service', 'establishments', 'may', 'open', 'shortages', 'experienced', 'key', 'items', 'support', 'restaurants', 'orders', 'found', 'list', 'https', 'www', 'capecodchamber', 'org', 'restaurants', 'restaurants', 'offering', 'takeout', 'check', 'elderly', 'neighbor', 'safe', 'distance', 'preferably', 'phone', 'see', 'well', 'need', 'supplies', 'please', 'stay', 'small', 'groups', 'practice', 'recommendations', 'cdc', 'self', 'https', 'bit', 'ly', 'qsmyun', 'aware', 'states', 'put', 'place', 'restrictions', 'related', 'coronavirus', 'covid', 'commonwealth', 'massachusetts', 'emergency', 'actions', 'posted', 'https', 'bit', 'ly', 'bszunl', 'call', 'massachusetts', 'state', 'supported', 'hotline', 'whether', 'second', 'home', 'choose', 'remain', 'primary', 'home', 'unprecedented', 'time', 'please', 'contribute', 'cape', 'cod', 'healthcare', 'https', 'bit', 'ly', 'lk', 'mx', 'donations', 'money', 'personal', 'protective', 'equipment', 'needed', 'local', 'healthcare', 'workers', 'protecting', 'family', 'provide', 'humanitarian', 'aid', 'local', 'individuals', 'families', 'many', 'without', 'jobs', 'guarantee', 'jobs', 'return', 'health', 'insurance', 'may', 'lose', 'housing', 'benefits', 'support', 'recommend', 'organizations', 'cape', 'cod', 'community', 'foundation', 'strategic', 'emergency', 'response', 'fund', 'https', 'www', 'capecodfoundation', 'org', 'community', 'response', 'covid', 'cape', 'cod', 'islands', 'major', 'crisis', 'relief', 'fund', 'https', 'majorcrisisrelieffund', 'org', 'cape', 'island', 'united', 'way', 'community', 'response', 'fund', 'https', 'www', 'capeandislandsuw', 'org', 'cape', 'cod', 'covid', 'workforce', 'housing', 'relief', 'fund', 'http', 'www', 'haconcapecod', 'org', 'together', 'weather', 'global', 'storm', 'wishing', 'peace', 'health', 'wendy', 'northcross', 'ceo', 'cape', 'cod', 'chamber', 'commerce', 'michael', 'lauf', 'ceo', 'john', 'yunits', 'county', 'administrator', 'barnstable', 'county', 'march letter', 'letter cape', 'cape cod', 'cod second', 'second homeowners', 'homeowners cape', 'cape cod', 'cod home', 'home year', 'year round', 'round residents', 'residents appreciate', 'appreciate depend', 'depend upon', 'upon seasonal', 'seasonal influx', 'influx visitors', 'visitors second', 'second homeowners', 'homeowners way', 'way life', 'life centuries', 'centuries coronavirus', 'coronavirus crisis', 'crisis understand', 'understand desire', 'desire second', 'second home', 'home cape', 'cape sheltering', 'sheltering place', 'place asking', 'asking please', 'please help', 'help us', 'us remain', 'remain safe', 'safe healthy', 'healthy following', 'following actions', 'actions individuals', 'individuals traveling', 'traveling cape', 'cape cod', 'cod cape', 'cape state', 'state self', 'self quarantine', 'quarantine days', 'days avoid', 'avoid spreading', 'spreading virus', 'virus bring', 'bring items', 'items need', 'need stay', 'stay including', 'including prescriptions', 'prescriptions groceries', 'groceries cleaning', 'cleaning supplies', 'supplies personal', 'personal health', 'health items', 'items personal', 'personal protective', 'protective equipment', 'equipment essential', 'essential service', 'service establishments', 'establishments may', 'may open', 'open shortages', 'shortages experienced', 'experienced key', 'key items', 'items support', 'support restaurants', 'restaurants orders', 'orders found', 'found list', 'list https', 'https www', 'www capecodchamber', 'capecodchamber org', 'org restaurants', 'restaurants restaurants', 'restaurants offering', 'offering takeout', 'takeout check', 'check elderly', 'elderly neighbor', 'neighbor safe', 'safe distance', 'distance preferably', 'preferably phone', 'phone see', 'see well', 'well need', 'need supplies', 'supplies please', 'please stay', 'stay small', 'small groups', 'groups practice', 'practice recommendations', 'recommendations cdc', 'cdc self', 'self https', 'https bit', 'bit ly', 'ly qsmyun', 'qsmyun aware', 'aware states', 'states put', 'put place', 'place restrictions', 'restrictions related', 'related coronavirus', 'coronavirus covid', 'covid commonwealth', 'commonwealth massachusetts', 'massachusetts emergency', 'emergency actions', 'actions posted', 'posted https', 'https bit', 'bit ly', 'ly bszunl', 'bszunl call', 'call massachusetts', 'massachusetts state', 'state supported', 'supported hotline', 'hotline whether', 'whether second', 'second home', 'home choose', 'choose remain', 'remain primary', 'primary home', 'home unprecedented', 'unprecedented time', 'time please', 'please contribute', 'contribute cape', 'cape cod', 'cod healthcare', 'healthcare https', 'https bit', 'bit ly', 'ly lk', 'lk mx', 'mx donations', 'donations money', 'money personal', 'personal protective', 'protective equipment', 'equipment needed', 'needed local', 'local healthcare', 'healthcare workers', 'workers protecting', 'protecting family', 'family provide', 'provide humanitarian', 'humanitarian aid', 'aid local', 'local individuals', 'individuals families', 'families many', 'many without', 'without jobs', 'jobs guarantee', 'guarantee jobs', 'jobs return', 'return health', 'health insurance', 'insurance may', 'may lose', 'lose housing', 'housing benefits', 'benefits support', 'support recommend', 'recommend organizations', 'organizations cape', 'cape cod', 'cod community', 'community foundation', 'foundation strategic', 'strategic emergency', 'emergency response', 'response fund', 'fund https', 'https www', 'www capecodfoundation', 'capecodfoundation org', 'org community', 'community response', 'response covid', 'covid cape', 'cape cod', 'cod islands', 'islands major', 'major crisis', 'crisis relief', 'relief fund', 'fund https', 'https majorcrisisrelieffund', 'majorcrisisrelieffund org', 'org cape', 'cape island', 'island united', 'united way', 'way community', 'community response', 'response fund', 'fund https', 'https www', 'www capeandislandsuw', 'capeandislandsuw org', 'org cape', 'cape cod', 'cod covid', 'covid workforce', 'workforce housing', 'housing relief', 'relief fund', 'fund http', 'http www', 'www haconcapecod', 'haconcapecod org', 'org together', 'together weather', 'weather global', 'global storm', 'storm wishing', 'wishing peace', 'peace health', 'health wendy', 'wendy northcross', 'northcross ceo', 'ceo cape', 'cape cod', 'cod chamber', 'chamber commerce', 'commerce michael', 'michael lauf', 'lauf ceo', 'ceo john', 'john yunits', 'yunits county', 'county administrator', 'administrator barnstable', 'barnstable county']\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"prep_token_nb\"] == df.prep_token_nb.max()][\"quotation\"].values[0])\n",
    "print(\"\\n\"*3)\n",
    "print(df[df[\"prep_token_nb\"] == df.prep_token_nb.max()][\"prep_quote\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DJARjEzmKB_"
   },
   "source": [
    "### Word to Vec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iki0ArDNmKB_"
   },
   "outputs": [],
   "source": [
    "total = []\n",
    "for prep in df.prep_quote.values:\n",
    "    total = total + list(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIYGLWrimKB_"
   },
   "outputs": [],
   "source": [
    "common = Counter(total).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmfNmXqUmKB_"
   },
   "outputs": [],
   "source": [
    "### don't run : really long\n",
    "### w2v.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5BJsv7gmKB_"
   },
   "outputs": [],
   "source": [
    "### be careful : tension on RAM\n",
    "model = w2v.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7SxEZyUmKB_",
    "outputId": "54d3f430-3764-4e6e-d180-668c6c98b632"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector(common[0][0]).reshape((1, 300)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sEXWZ13mKB_",
    "outputId": "333bf84b-70f4-416c-9ee6-fdcb01222981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 336.03 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10.7 ms ± 23.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 w2v.aggregate(model, df.prep_quote.values[4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLLrATR-mKCA",
    "outputId": "65d54257-3baa-4a50-de98-8eb9c157aebc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>quote_len</th>\n",
       "      <th>prep_quote</th>\n",
       "      <th>prep_token_nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Why now, 2021? Here's why,</td>\n",
       "      <td>Michael J. Graham</td>\n",
       "      <td>2020-03-02 21:03:20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14178</th>\n",
       "      <td>should be out in 2020</td>\n",
       "      <td>Kate Hudson</td>\n",
       "      <td>2020-02-17 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16443</th>\n",
       "      <td>not having the same face</td>\n",
       "      <td>Angelina Pivarnick</td>\n",
       "      <td>2020-02-05 23:38:27</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20738</th>\n",
       "      <td>He can take him on.</td>\n",
       "      <td>Ray Newman</td>\n",
       "      <td>2020-02-12 01:06:39</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49164</th>\n",
       "      <td>I didn't want that out,</td>\n",
       "      <td>Clayton Kershaw</td>\n",
       "      <td>2020-04-15 12:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        quotation             speaker                 date  \\\n",
       "9995   Why now, 2021? Here's why,   Michael J. Graham  2020-03-02 21:03:20   \n",
       "14178       should be out in 2020         Kate Hudson  2020-02-17 00:00:00   \n",
       "16443    not having the same face  Angelina Pivarnick  2020-02-05 23:38:27   \n",
       "20738         He can take him on.          Ray Newman  2020-02-12 01:06:39   \n",
       "49164     I didn't want that out,     Clayton Kershaw  2020-04-15 12:00:00   \n",
       "\n",
       "       numOccurrences  quote_len prep_quote  prep_token_nb  \n",
       "9995                1          9         []              0  \n",
       "14178               5          5         []              0  \n",
       "16443               1          5         []              0  \n",
       "20738               1          6         []              0  \n",
       "49164               6          7         []              0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.prep_token_nb == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gpc_VldmKCA"
   },
   "outputs": [],
   "source": [
    "df = df[df.prep_token_nb != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpKhrWlSmKCB"
   },
   "outputs": [],
   "source": [
    "## Comme dirait Jean Pierre Coff : C DE LA MERDE\n",
    "\n",
    "X = w2v.get_w2c_matrix(model, df, \"prep_quote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrJQX9MVmKCB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz.show_w2v_words(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RmdzTkCmKCB"
   },
   "source": [
    "### Discrimanation between years :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D24tQdczmKCB"
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADouwyXmmKCC",
    "outputId": "cdffb94e-4886-400d-e6bc-0122e0423a1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-108-1b9000f2df69>:3: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  split_dates.append(pd.datetime(2020,month_id,1))\n",
      "<ipython-input-108-1b9000f2df69>:4: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  split_dates.append(pd.datetime(2021, 1, 1))\n"
     ]
    }
   ],
   "source": [
    "split_dates = []\n",
    "for month_id in range(1, 13):\n",
    "    split_dates.append(pd.datetime(2020,month_id,1))\n",
    "split_dates.append(pd.datetime(2021, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkQUsKB9mKCC"
   },
   "source": [
    "### Final Pipeline + Benchmarking :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBd1_xyomKCC"
   },
   "outputs": [],
   "source": [
    "def benchmark(start, part):\n",
    "    print(f\"It took for 10_000 samples : {round(time() - start, 2)} to {part}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miSbpMolmKCC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "model = w2v.get_model()\n",
    "benchmark(start, \"load model\")\n",
    "\n",
    "startr = time()\n",
    "df = helpers.get_samples(num_samples=20_000, random=True)\n",
    "benchmark(startr, \"load data\")\n",
    "\n",
    "df = df.drop([\"qids\", \"probas\", \"phase\", \"quoteID\", \"urls\"], axis=1) ## get rid of useless cols\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]) ## need date to split it after (need proper typing)\n",
    "\n",
    "startr = time()\n",
    "df[\"prep_quote\"] = df.quotation.apply(tt.preprocess_quote) ## preprocess quotes\n",
    "benchmark(startr, \"preprocess\")\n",
    "\n",
    "## discriminate with the month here :\n",
    "df[\"month\"] = pd.DatetimeIndex(df[\"date\"]).month\n",
    "\n",
    "## adds random sentiment\n",
    "fake_sentiments = np.random.randint(0, 2, len(df.index))\n",
    "df['sentiment'] = fake_sentiments\n",
    "\n",
    "startr = time()\n",
    "## get all the datapoints (one per quote) in W2V vector space\n",
    "vec_spaces, labels = zip(*df.groupby(\"month\").apply(lambda x : w2v.get_w2c_matrix(model, x, \"prep_quote\", \"sentiment\")).values)\n",
    "benchmark(startr, \"get matrices\")\n",
    "\n",
    "## plot them all : \n",
    "startr = time()\n",
    "[viz.show_w2v_words(vec_space, outfilename=f'W2V{idx}.png', colors=viz.get_cmap_from_labels(labels[idx])) for idx, vec_space in enumerate(vec_spaces)]\n",
    "benchmark(startr, \"plot\")\n",
    "\n",
    "benchmark(start, \"do everything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmlPYFvImKCC",
    "outputId": "b2ce6466-a6b7-4966-c3a0-7628692e7eb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.46200e-03, 4.66000e-04, 1.38660e-02, 1.00000e+00],\n",
       "       [1.46200e-03, 4.66000e-04, 1.38660e-02, 1.00000e+00],\n",
       "       [9.87053e-01, 9.91438e-01, 7.49504e-01, 1.00000e+00],\n",
       "       ...,\n",
       "       [1.46200e-03, 4.66000e-04, 1.38660e-02, 1.00000e+00],\n",
       "       [1.46200e-03, 4.66000e-04, 1.38660e-02, 1.00000e+00],\n",
       "       [1.46200e-03, 4.66000e-04, 1.38660e-02, 1.00000e+00]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.get_cmap_from_labels(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyioS-oHmKCC"
   },
   "source": [
    "### Evaluate the clusterizations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isRL_fdXmKCC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_spaces = df.groupby(\"month\").apply(lambda x : helpers.get_w2c_matrix(model, x, \"prep_quote\"))\n",
    "[helpers.show_w2v_words(vec_space, outfilename=f'W2V{idx}.png', colors=helpers.get_color_map(df, \"sentiment\")) for idx, vec_space in enumerate(vec_spaces)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9-qk4b6mKCC",
    "outputId": "65a8fedc-f982-4f4d-d515-4e2d072174c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\antom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "reload(helpers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU7YoM85mKCC"
   },
   "outputs": [],
   "source": [
    "### we study January\n",
    "jvec_space = vec_spaces[0]\n",
    "jlabels = labels[0]\n",
    "pos_vecs = jvec_space[jlabels == 1]\n",
    "neg_vecs = jvec_space[jlabels == 0]\n",
    "\n",
    "cpos = helpers.get_center_of_mass(pos_vecs)\n",
    "cneg = helpers.get_center_of_mass(neg_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfF4TSO3mKCC",
    "outputId": "56a0915e-0b05-4ca2-c79c-765330e56bd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antom\\Documents\\LIFE\\EPFL\\MA\\MA1\\ADA\\ada-2021-project-adada-sur-mon-bidet\\helpers.py:183: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.999817626607963"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.normalized_cut(pos_vecs, neg_vecs, helpers.cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anqyqsLsmKCC",
    "outputId": "94253917-0a9b-40e5-c007-25232c38d147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5503, 300)\n",
      "(5503,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antom\\Documents\\LIFE\\EPFL\\MA\\MA1\\ADA\\ada-2021-project-adada-sur-mon-bidet\\helpers.py:183: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with month 1\n",
      "(5007, 300)\n",
      "(5007,)\n",
      "Done with month 2\n",
      "(4159, 300)\n",
      "(4160,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 4159 but corresponding boolean dimension is 4160",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-319-ba6a87d4f57e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mjlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpos_vecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjvec_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjlabels\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mneg_vecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjvec_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjlabels\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 4159 but corresponding boolean dimension is 4160"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for month in range(4):    \n",
    "    jvec_space = vec_spaces[month]\n",
    "    print(vec_spaces[month].shape)\n",
    "    print(labels[month].shape)\n",
    "    jlabels = labels[month]\n",
    "    pos_vecs = jvec_space[jlabels == 1]\n",
    "    neg_vecs = jvec_space[jlabels == 0]\n",
    "\n",
    "    cpos = helpers.get_center_of_mass(pos_vecs)\n",
    "    cneg = helpers.get_center_of_mass(neg_vecs)\n",
    "    stats.append([helpers.cosine_sim(cpos, cneg), helpers.normalized_cut(pos_vecs, neg_vecs)])\n",
    "    print(f\"Done with month {month + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoxeOFE1mKCD"
   },
   "source": [
    "#### Benchmark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zj6qyb7WmKCD"
   },
   "outputs": [],
   "source": [
    "sizes = 10 ** np.arange(2, 6)\n",
    "stats = []\n",
    "#model = helpers.get_model()\n",
    "for size in sizes:\n",
    "    reps = []\n",
    "    print(f\"Starting size : {size}\")\n",
    "    for rep in range(10):\n",
    "        df = get_samples(num_samples=size, random=True)\n",
    "        try :\n",
    "            reps.append(helpers.process(df, model))\n",
    "        except :\n",
    "            print(\"Failed\")\n",
    "    stats.append(np.mean(reps))\n",
    "    print(f\"Done for size : {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AemaFZumKCD"
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots(figsize=(7, 4))\n",
    "plt.plot(stats)\n",
    "plt.plot(n_stats)\n",
    "a.set_xticklabels([\"\",\"1e2\",\"\", \"1e3\",\"\", \"1e4\", \"\", \"1e5\"])\n",
    "a.set_ylabel(\"runtime [s]\")\n",
    "a.set_xlabel(\"number of samples\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsEFf-vQmKCD"
   },
   "outputs": [],
   "source": [
    "n_stats = []\n",
    "## parallelized benchmark\n",
    "sizes = 10 ** np.arange(2, 6)\n",
    "for size in sizes:\n",
    "    reps = []\n",
    "    print(f\"Starting size : {size}\")\n",
    "    for rep in range(5):\n",
    "        df = get_samples(num_samples=size, random=True)\n",
    "        try :\n",
    "            reps.append(helpers.process(df, model, par=False))\n",
    "        except :\n",
    "            print(\"Failed\")\n",
    "    n_stats.append(np.mean(reps))\n",
    "    print(f\"Done for size : {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EE2fDrLomKCD",
    "outputId": "a02247aa-30c3-4f9b-d07a-74ce1573eb1f"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "process() got an unexpected keyword argument 'par'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-dba3c8b70ef6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: process() got an unexpected keyword argument 'par'"
     ]
    }
   ],
   "source": [
    "df = get_samples(num_samples=100, random=True)\n",
    "helpers.process(df, model, par=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJHN7QWUmKCD"
   },
   "source": [
    "# Wikipedia feature engineering\n",
    "\n",
    "The wikipedia part showcases our capabilities to extract features from Wikipedia using QIDs. Such features are gender, political assignation or age. Wikipedia data is quite messy and the heuristics used to extract these features are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haEWpaelmKCD",
    "outputId": "32f8f640-d013-47c0-97d1-7bdb8070edca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lucastrg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lucastrg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/home/lucastrg/FLEP/MA1/ADA/ada-2021-project-adada-sur-mon-bidet/./helpers/helpers.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "sys.path.append('./helpers/')\n",
    "sys.path.append('./feature_engineering/')\n",
    "import names\n",
    "import helpers\n",
    "\n",
    "reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-qqSTTvmKCD"
   },
   "source": [
    "## Pre-Processing\n",
    "We will remove all the quotes without a speaker, and we will extract the set of all the speakers and QIDs of the sampled rows.\n",
    "We then fetch a json of each speaker's whole page as well as all its PIDs and RIDs (these 2 IDs are not yet in use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kc9IJALKmKCD"
   },
   "outputs": [],
   "source": [
    "df = helpers.get_samples(num_samples=10000, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00b8z4FzmKCD"
   },
   "outputs": [],
   "source": [
    "df=df[df[\"speaker\"]!=\"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2W5fmDEmKCE",
    "outputId": "97b7a618-cba7-4297-d80b-e7970e5c82dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5978"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBUHfrPRmKCE"
   },
   "source": [
    "Not so bad ! About 60% of the rows are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fq_C5uZymKCE"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08ZO0QFAmKCE"
   },
   "outputs": [],
   "source": [
    "qids=list(set(df[\"qids\"].to_numpy().sum()))\n",
    "speakers=list(set(df[\"speaker\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-N5RNPtmKCE",
    "outputId": "e9f7475a-ae21-41ba-ad0b-6f41e388c0b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fe_qXCC9mKCE"
   },
   "outputs": [],
   "source": [
    "request_template= \"https://www.wikidata.org/wiki/Special:EntityData/{}.json\"\n",
    "request_template2=\"https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&titles={}&formatversion=2&rvprop=content&rvslots=*\"\n",
    "request_template3=\"https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&pageids={}&formatversion=2&rvprop=content&rvslots=*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qStol4QGmKCE"
   },
   "outputs": [],
   "source": [
    "invalid_qids=[]\n",
    "for qid in qids[:10]:\n",
    "   try :\n",
    "      with urllib.request.urlopen(request_template.format(qid)) as response:\n",
    "         raw_data = response\n",
    "         data = json.load(raw_data)\n",
    "         data.keys()\n",
    "   except urllib.request.HTTPError :\n",
    "      invalid_qids.append(qid)\n",
    "\n",
    "\n",
    "      #print(json.dumps(data, indent=2, sort_keys=True))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqCZKYSumKCE"
   },
   "source": [
    "Doesn't work atm, not really useful so might drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybO4AhYgmKCE",
    "outputId": "a55129aa-80b1-49c4-e6d6-4f0e18bd3be1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qid_to_rid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7337/2263037819.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0;32mtry\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_template3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqid_to_rid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m          \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qid_to_rid' is not defined"
     ]
    }
   ],
   "source": [
    "invalid_qids=[]\n",
    "for qid in qids[:10]:\n",
    "   try :\n",
    "      with urllib.request.urlopen(request_template3.format(qid_to_rid[qid])) as response:\n",
    "         raw_data = response\n",
    "         data = json.load(raw_data)\n",
    "         data.keys()\n",
    "   except urllib.request.HTTPError :\n",
    "      invalid_qids.append(qid)\n",
    "\n",
    "\n",
    "      #print(json.dumps(data, indent=2, sort_keys=True))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC68wRWqmKCE",
    "outputId": "fe086165-b344-44c9-e7b3-fd4b338dbb91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchcomplete': True,\n",
       " 'query': {'pages': [{'pageid': 1426072190, 'missing': True}]}}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SWicNbtmKCE"
   },
   "source": [
    "#Wikipedia data fetching\n",
    "Fetches all we need to know about a speaker (using their name). Handles one redirection if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eorJZlXBmKCG",
    "outputId": "8f7f9444-cca3-430d-a76d-c8b9d0f98f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirect  Clara Kramer -> Clara's War\n",
      "Redirect  Jeffrey Mims -> D. Jeffrey Mims\n",
      "Redirect  Cesar Diaz -> César Díaz\n",
      "Redirect  Matty Healy -> The \n",
      "Redirect  Stephane Dujarric -> Stéphane Dujarric\n",
      "Redirect  Darion Anderson -> Jake Anderson \n",
      "Redirect  Joe Giudice -> Teresa Giudice\n",
      "Redirect  Bob Miller -> Robert Miller\n",
      "Redirect  V Srinivasan -> V. Srinivasan\n",
      "Redirect  Mick Cronin -> Michael Cronin\n",
      "Redirect  Bill O'Brien -> William O'Brien \n",
      "Redirect  Bobby James -> Bob James\n",
      "Redirect  Kim Kardashian West -> Kim Kardashian\n",
      "Redirect  Bill Chapman -> William Chapman\n",
      "Redirect  Georgina Wood -> Georgina Theodora Wood\n",
      "Redirect  Kareena Kapoor Khan -> Kareena Kapoor\n",
      "Redirect  Mike White -> Michael White\n",
      "Redirect  Dave Roberts -> David Roberts\n",
      "Redirect  Sinead O'Connor -> Sinéad O'Connor\n",
      "Redirect  Mike Green -> Michael Green\n",
      "Redirect  Bill Hoffman -> William Hoffman\n",
      "Redirect  Tedros Adhanom Ghebreyesus -> Tedros Adhanom\n",
      "Redirect  Danny Garcia -> Daniel García\n",
      "Redirect  Stephen Townsend -> Stephen J. Townsend\n",
      "Redirect  Matt Lee -> Matthew Lee\n",
      "Redirect  Mike Dean -> Michael Dean\n",
      "Redirect  Jim Davis -> James Davis\n",
      "Redirect  Queen Elizabeth II -> Elizabeth II\n",
      "Redirect  Jurgen Klopp -> Jürgen Klopp\n",
      "Redirect  Ken Clarke -> Kenneth Clarke\n",
      "Redirect  C. Smith -> List of people with surname Smith\n",
      "Redirect  Barbora Strycova -> Barbora Strýcová\n",
      "Redirect  A.B. Quintanilla III -> A. B. Quintanilla\n",
      "Redirect  Mike Lee -> Michael Lee\n",
      "Redirect  Katinka Hosszu -> Katinka Hosszú\n",
      "Redirect  Stergomena Lawrence Tax -> Stergomena Tax\n",
      "Redirect  Dr Chidi -> Chidi Ngwaba\n",
      "Redirect  Lawrence S. Bacow -> Lawrence Bacow\n",
      "Redirect  Sean Wright -> The Contender Asia\n",
      "Redirect  Daniel Grossman -> Danny Grossman\n",
      "Redirect  Jack Cunningham -> Jack Cunningham\n",
      "Redirect  Patricia De Lille -> Patricia de Lille\n",
      "Redirect  Recep Tayyip Erdogan -> Recep Tayyip Erdo\n",
      "Redirect  Francois Legault -> François Legault\n",
      "Redirect  Josh Jones -> Joshua Jones\n",
      "Redirect  Bill Lee -> William Lee\n",
      "Redirect  Mike Smith -> Michael Smith\n",
      "Redirect  Xi Xiaoxing -> Xiaoxing Xi\n",
      "Redirect  Charles Lieber -> Charles M. Lieber\n",
      "Redirect  Binyamin Netanyahu -> Benjamin Netanyahu\n",
      "Redirect  Joel Simon -> Committee to Protect Journalists\n",
      "Redirect  Felix Rivera -> Anchorage\n",
      "Redirect  Chris Jones -> Christopher Jones\n",
      "Redirect  Dan Lewis -> Daniel Lewis\n",
      "Redirect  Vimal Kumar -> U. Vimal Kumar\n",
      "Redirect  Stephen Bennett -> Steve Bennett\n",
      "Redirect  Iron Lady -> Margaret Thatcher\n",
      "Redirect  Mike Johnson -> Michael Johnson\n",
      "Redirect  TJ Warren -> T. J. Warren\n",
      "Redirect  Ken Griffey -> Ken Griffey Jr.\n",
      "Redirect  dos Santos -> Santos \n",
      "Redirect  Robin Skinner -> Cavetown\n",
      "Redirect  Mike Scott -> Michael Scott\n",
      "Redirect  Daniel Hemmert -> Dan Hemmert\n",
      "Redirect  Joe White -> Joseph White\n",
      "Redirect  Paul Levy -> Paul Lévy\n",
      "Redirect  Adama Traore -> Adama Traoré\n",
      "Redirect  Rebii Simon -> Rebeka Simon\n",
      "Redirect  Martin Perez -> Martín Pérez\n",
      "Redirect  Sonia Lopez -> Sonia López\n",
      "Redirect  Van Buren -> Martin Van Buren\n",
      "Redirect  Bong Joon Ho -> Bong Joon-ho\n",
      "Redirect  Bing Liu -> Liu Bing\n",
      "Redirect  Tony Fauci -> Anthony Fauci\n",
      "Redirect  Alex Ovechkin -> Alexander Ovechkin\n",
      "Redirect  Dave Evans -> David Evans\n",
      "Redirect  Young Lee -> Pinkberry\n",
      "Redirect  Bobby Jones -> Robert Jones\n",
      "Redirect  Jonathan Jenkins -> Jon Jenkins \n",
      "Redirect  Michael McKenzie -> Michael MacKenzie\n",
      "Redirect  Andrew Cross -> Andrew Crosse\n",
      "Redirect  Benoit Laliberte -> Benoît Laliberté\n",
      "Redirect  Gerardo Gonzalez -> Giraldo González de la Renta\n",
      "Redirect  Mike Armstrong -> Michael Armstrong\n",
      "Redirect  Santa Ono -> Santa J. Ono\n",
      "Redirect  Bandy Lee -> Bandy X. Lee\n",
      "Redirect  Sir Patrick Stewart -> Patrick Stewart\n",
      "Redirect  Dave Williams -> David Williams\n",
      "Redirect  Kevin Green -> Kevin Greene\n",
      "Redirect  Dr. Robert -> Doctor Robert\n",
      "Redirect  Bob Bowman -> Robert Bowman\n",
      "Redirect  Chloe Dygert -> Chloé Dygert\n",
      "Redirect  Hillary Rodham Clinton -> Hillary Clinton\n",
      "Redirect  Mike Ryan -> Michael Ryan\n",
      "Redirect  Dan Williams -> Daniel Williams\n",
      "Redirect  Joe Ryan -> Joseph Ryan\n",
      "Redirect  YBN Cordae -> Cordae\n",
      "Redirect  Mike Ramsey -> Michael Ramsey \n",
      "Redirect  Qiang Wang -> Wang Qiang\n",
      "Redirect  Andre Gomes -> André Gomes\n",
      "Redirect  Mike Russell -> Michael Russell\n",
      "Redirect  Bill Ferguson -> William Ferguson\n",
      "Redirect  Rick Wilson -> Richard Wilson\n",
      "Redirect  Sean Cummins -> Seán Cummins\n",
      "Redirect  Branko Milanovic -> Branko Milanovi\n",
      "Redirect  Quique Setien -> Quique Setién\n",
      "Redirect  Herizen Guardiola -> Herizen F. Guardiola\n",
      "Redirect  Jillian Saulnier -> Jill Saulnier\n",
      "Redirect  President Bill Clinton -> Bill Clinton\n",
      "Redirect  Tom Kennedy -> Thomas Kennedy\n",
      "Redirect  Tom Reed -> Thomas Reed\n",
      "Redirect  Russell Brock -> Russell Brock\n",
      "Redirect  France Córdova -> France A. Córdova\n",
      "Redirect  Jose Mourinho -> José Mourinho\n",
      "Redirect  Oscar Pujol -> Óscar Pujol\n",
      "Redirect  Alex Fletcher -> Alexander Fletcher\n",
      "Redirect  Tiffany Lee -> Plumb \n",
      "Redirect  Bobby Scott -> Robert Scott\n",
      "Redirect  Larry Birkhead -> Dannielynn Birkhead paternity case\n",
      "Redirect  Shad Khan -> Shahid Khan\n",
      "Redirect  Frederik Madsen -> Frederik Rodenberg\n",
      "Redirect  Tom Reilly -> Thomas Reilly \n",
      "Redirect  Mike McKenna -> Michael McKenna\n",
      "Redirect  Vicente Navarro -> Vicenç Navarro\n",
      "Redirect  Anant Singh -> Anant Kumar Singh\n",
      "Redirect  MK Stalin -> M. K. Stalin\n",
      "Redirect  ALICE COOPER -> Alice Cooper\n",
      "Redirect  Meghan Markle -> Meghan\n",
      "Redirect  Jon Edwards -> Jonathan Edwards\n",
      "Redirect  Mike Kelly -> Michael Kelly\n",
      "Redirect  Eleider Alvarez -> Eleider Álvarez\n",
      "Redirect  Kathryn Murray -> Arthur Murray\n",
      "Redirect  Antonio Ortiz -> Antonio Ortiz Ramírez\n",
      "Redirect  Vinton Cerf -> Vint Cerf\n",
      "Redirect  Bryan Perez -> Bryan Pérez\n"
     ]
    }
   ],
   "source": [
    "invalid_speakers=[]\n",
    "speaker_content={}\n",
    "for speaker in speakers[:2000]:\n",
    "   try :\n",
    "      with urllib.request.urlopen(request_template2.format(urllib.parse.quote(speaker))) as response:\n",
    "         raw_data = json.load(response)[\"query\"][\"pages\"][0]\n",
    "         \n",
    "         if raw_data.get(\"missing\",False):\n",
    "            invalid_speakers.append(speaker)\n",
    "         else:\n",
    "            content = raw_data[\"revisions\"][0][\"slots\"][\"main\"][\"content\"]\n",
    "            if re.search(\"^'''{}''' may refer to\".format(speaker),content): #Drop disambiguation pages\n",
    "               invalid_speakers.append(speaker)\n",
    "\n",
    "            else:\n",
    "               if re.search(\"(^#REDIRECT \\[\\[)([A-Za-z 'À-ÿZİı.-]*)\", content): #Allows to fix most redirecting problems \n",
    "                  speaker_alt = re.search(\"(^#REDIRECT \\[\\[)([A-Za-z 'À-ÿZİı.-]*)\", content).group(2)\n",
    "                  print(\"Redirect \", speaker ,\"->\",speaker_alt) #Je laisse le print parce qu'il est satisfaisant\n",
    "                  if speaker_alt:\n",
    "                     with urllib.request.urlopen(request_template2.format(urllib.parse.quote(speaker_alt))) as response:\n",
    "                        raw_data = json.load(response)[\"query\"][\"pages\"][0]\n",
    "                        if raw_data.get(\"missing\",False):\n",
    "                           invalid_speakers.append(speaker)\n",
    "                        else:\n",
    "                           content = raw_data[\"revisions\"][0][\"slots\"][\"main\"][\"content\"]\n",
    "                  else :\n",
    "                     content = \"ERROR\"\n",
    "               speaker_content[raw_data[\"title\"]]=content\n",
    "            \n",
    "   except urllib.request.HTTPError :\n",
    "      invalid_speakers.append(speaker)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNvziszOmKCG",
    "outputId": "ae5535cb-9255-4477-cf91-dc44f406f4e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speaker_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKY5ADsjmKCG"
   },
   "source": [
    "We manage to fetch around 75% of the wikipedia page that we were looking for ! \n",
    "However we can notice a small percentage of rows that are considered as valid to be completely wrong. Since we fetch the jsons using the name of the speaker, we can either have trouble resolving homonyms, or simply suffer from badly assigned names (i.e. \"Theater Director\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjj7VDCNmKCH"
   },
   "source": [
    "## Political Side assignation\n",
    "Here we're guessing the political side of each speaker with somewhat good accuracy. We use 2 different strategies, if the speaker has a well filled in wikipedia page, we can simply find its current political party. If not, we're using a surprisingly alright heuristic, we simply count the occurences of words assigned to democrats (i.e. \"left-wing\", \"liberal\", ...) and republicans, and compare the 2 counts.\n",
    "\n",
    "NB: There is obviously one major assumption that speaker belong exclusively to either of these two (or none). However, even in the US, some speakers are \"in the middle\". \n",
    "\n",
    "It should also be noted that some speakers are not American, we however found that our heuristic still matched those speakers with conservatives view to the Republican and vice-versa). We shall in the next milestone investigate further and perhaps adopt a deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9ZS797kmKCH"
   },
   "outputs": [],
   "source": [
    "def pol_compass_from_wiki(speakers_content, discrete = True):\n",
    "    if discrete:\n",
    "        dem_words=[\"democrat\", \"left-wing\", \"liberal\"]\n",
    "        rep_words =[\"republican\", \"conservative\", \"right-wing\"]\n",
    "\n",
    "\n",
    "        for speaker in speakers_content:\n",
    "            yielded = False\n",
    "            s= speakers_content[speaker].lower()\n",
    "\n",
    "            for line in s.split(\"\\n\"):\n",
    "                if \"| party\" in line:\n",
    "                    if any(x in line for x in dem_words):\n",
    "                        yield speaker, (\"Democrat\", -1)\n",
    "                        yielded = True\n",
    "                    elif any(x in line for x in rep_words):\n",
    "                        yielded = True\n",
    "                        yield speaker, (\"Republican\", -1)\n",
    "                \n",
    "            if not yielded:\n",
    "\n",
    "                dem= sum(s.count(x) for x in dem_words)\n",
    "                rep= sum(s.count(x) for x in rep_words)\n",
    "                total = rep+dem\n",
    "                if total:\n",
    "                    yield speaker, (\"Democrat\" if dem>rep else \"Republican\", total)\n",
    "    else:\n",
    "        dem_words=[\"democrat\", \"left-wing\", \"liberal\"]\n",
    "        rep_words =[\"republican\", \"conservative\", \"right-wing\"]\n",
    "\n",
    "        for speaker in speakers_content:\n",
    "            yielded = False\n",
    "            s= speakers_content[speaker].lower()\n",
    "\n",
    "            for line in s.split(\"\\n\"):\n",
    "                if \"| party\" in line:\n",
    "                    if any(x in line for x in dem_words):\n",
    "                        yield speaker, (1,0, -1)\n",
    "                        yielded = True\n",
    "                    elif any(x in line for x in rep_words):\n",
    "                        yielded = True\n",
    "                        yield speaker, (0,1, -1)\n",
    "                \n",
    "            if not yielded:\n",
    "\n",
    "                dem= sum(s.count(x) for x in dem_words)\n",
    "                rep= sum(s.count(x) for x in rep_words)\n",
    "                total = rep+dem\n",
    "                if total:\n",
    "                    yield speaker, (dem/total,rep/total, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saaT_wEwmKCH",
    "outputId": "07af73af-d2d0-4f9d-bd9a-f4938a781316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_wing= dict(pol_compass_from_wiki(speaker_content))\n",
    "len(speaker_wing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfGrl27QmKCH"
   },
   "source": [
    "## Gender assignation\n",
    "\n",
    "In order to guess the gender if the speakers, we again use 2 strategies. At first, we try guessing the gender by counting occurences of gendered pronoums, but if we don't get any, we train a classifier, which solely uses the name of the speaker to guess the gender (thus with pretty bad accuracy ~70%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNlFdIkkmKCH",
    "outputId": "568f5e0d-9652-41e2-a6be-fa13e06041c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/lucastrg/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier as NBC\n",
    "from nltk import classify\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMpJqeG1mKCH"
   },
   "source": [
    "For the classifier we use both the whole name as well as only the last letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtpniqjXmKCI"
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {\"whole name\" : word, \"lastletter\" : word[-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vNNdtyfmKCI"
   },
   "source": [
    "Training set loading and parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dU7YeX7dmKCI"
   },
   "outputs": [],
   "source": [
    "femaleNames = [ (name, \"female\") for name in names.words(\"female.txt\") ]\n",
    "maleNames = [ (name, \"male\") for name in names.words(\"male.txt\") ]\n",
    "allNames = maleNames + femaleNames\n",
    "random.shuffle(allNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXPlMv1LmKCI"
   },
   "source": [
    "Actually training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t19xOVkvmKCI"
   },
   "outputs": [],
   "source": [
    "featureData = [(gender_features(namelist), gender) for (namelist, gender) in allNames ]\n",
    "test_data = featureData[:500]\n",
    "train_data = featureData[500:]\n",
    "classifier = NBC.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi0AZ06UmKCI"
   },
   "outputs": [],
   "source": [
    "def gender_from_wiki(speaker_content):\n",
    "    he_words=[\" he \", \" him\", \" him\"] #The spaces are important, don't modify\n",
    "    she_words =[\" she \", \" her\"]\n",
    "    they_words=[\" they \", \" them\"]\n",
    "\n",
    "    for speaker in speaker_content:\n",
    "        s= speaker_content[speaker].lower()\n",
    "\n",
    "        he= sum(s.count(x) for x in he_words)\n",
    "        she= sum(s.count(x) for x in she_words)\n",
    "        they= sum(s.count(x) for x in they_words)\n",
    "        total = he+she+they\n",
    "\n",
    "        if True:\n",
    "            if total==0:\n",
    "                 yield (speaker, classifier.classify(gender_features(speaker.split()[0])))\n",
    "            elif he == max(he,she,they):\n",
    "                yield(speaker, \"male\")\n",
    "            elif she == max(he,she,they):\n",
    "                yield(speaker, \"female\")\n",
    "            else:\n",
    "                yield(speaker, \"other\") \n",
    "        if False and total:\n",
    "            yield speaker, (he/total,she/total, they/total, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mux6FY2JmKCI",
    "outputId": "24daa073-10bb-4602-ec09-8e5784faa6f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_gender = dict(gender_from_wiki(speaker_content))\n",
    "len(speaker_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zONZeiVamKCI",
    "outputId": "5f6a29fc-b75d-4612-fdbb-4fa8e0e08dfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['female', 'male', 'other'], dtype='<U6'), array([ 364, 1254,    4]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = []\n",
    "for speaker, gender in speaker_gender.items():\n",
    "    tmp.append(gender)\n",
    "np.unique(tmp, return_counts=True) #Snif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2_4ILcGmKCI"
   },
   "source": [
    "As we can see, only 20% of the speakers, are female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8G8xHrymKCI"
   },
   "source": [
    "## Age assignation\n",
    "Much easier to do, we can most of the time get a solid birth date and compute the age of the speaker (not precisely, we only use the year, since we're more interested about seeing general trends rather than precise assignation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DBZE16PmKCI"
   },
   "outputs": [],
   "source": [
    "def age_from_wiki(speaker_content):\n",
    "    count = 0\n",
    "\n",
    "    for speaker in speaker_content:\n",
    "        s= speaker_content[speaker].lower().split(\"\\n\")\n",
    "        for line in s:\n",
    "            if \"birth_date\" in line:\n",
    "                    \n",
    "                    match = re.match(\"^(\\|birth_date={{birthdateandage\\|(\\w*=\\w*\\|)?)([0-9]*)\\|([0-9]*)\\|([0-9]*)\", line.replace(\" \",\"\"))\n",
    "                    if match:\n",
    "                        age = 2022-int(match.group(3))\n",
    "                        yield(speaker, age)\n",
    "                    else : \n",
    "                        match = re.match(\"^(\\|birth_date={{birthdate\\|(\\w*=\\w*\\|)?)([0-9]*)\\|([0-9]*)\\|([0-9]*)\", line.replace(\" \",\"\"))\n",
    "                        if match:\n",
    "                            age = 2022-int(match.group(3))\n",
    "                            yield(speaker, age)\n",
    "                        else :\n",
    "                            match = re.match(\"^(\\|birth_date={{birthyearandage\\|(\\w*=\\w*\\|)?)([0-9]*)\", line.replace(\" \",\"\"))\n",
    "                            if match:\n",
    "                                age = 2022-int(match.group(3))\n",
    "                                yield(speaker, age)     \n",
    "                            else:\n",
    "                                count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsyH_zAsmKCJ"
   },
   "outputs": [],
   "source": [
    "speaker_age = dict(age_from_wiki(speaker_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U8vuhbomKCJ",
    "outputId": "7a228bbd-d7c0-4157-b192-e6e3919c3285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speaker_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_jotl3amKCJ"
   },
   "outputs": [],
   "source": [
    "plt.hist(speaker_age.values(), bins=20)\n",
    "plt.title(\"Empirical age distribution of the sampled speakers (without filtering)\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlJWeMcumKCJ"
   },
   "outputs": [],
   "source": [
    "wing = []\n",
    "ages = []\n",
    "gender = []\n",
    "\n",
    "\n",
    "for speaker in speaker_content:\n",
    "    if speaker in speaker_age.keys() and speaker in speaker_gender.keys() and speaker in speaker_wing.keys() and speaker_age[speaker]<120:\n",
    " \n",
    "        ages.append(speaker_age[speaker])\n",
    "        wing.append(speaker_wing[speaker][0])\n",
    "        gender.append(speaker_gender[speaker])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh34vIcdmKCJ"
   },
   "outputs": [],
   "source": [
    "plt.hist(wing)\n",
    "plt.title(\"Observed repartition between the 2 parties\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwkSxEXWmKCJ"
   },
   "source": [
    "In order to get a more precise view of the age of speakers that could have contributed to the climate change question, we filtered speakers over 120 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgXIcy7WmKCJ"
   },
   "outputs": [],
   "source": [
    "plt.hist(ages,bins=20)\n",
    "plt.title(\"Empirical age distribution of the sampled speakers (with a bit of filtering)\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1ss9jGhmKCJ"
   },
   "outputs": [],
   "source": [
    "big_dict={}\n",
    "for speaker in speaker_content:\n",
    "    if speaker in speaker_age.keys() and speaker in speaker_gender.keys() and speaker in speaker_wing.keys() and speaker_age[speaker]<120:\n",
    "        big_dict[speaker]=(speaker_age[speaker], speaker_gender[speaker], speaker_wing[speaker][0],speaker_wing[speaker][1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a43_DCrmKCJ"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(big_dict, orient=\"index\", columns=[\"age\", \"gender\", \"wing\", \"political_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHs_0QtcmKCJ"
   },
   "outputs": [],
   "source": [
    "df.wing = df.wing.astype( \"category\")\n",
    "df.gender = df.gender.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YR4LdCVmKCK"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,12)})\n",
    "sns.catplot(x=\"wing\", y=\"age\", hue=\"gender\", kind=\"swarm\", data=df, height=9).fig.suptitle(\"Age and gender distribution for each major political wing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuBQqqYEmKCK"
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"age\", hue=\"wing\").set_title(\"Age distribution of each major political wing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Kh2Q32CmKCK"
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"age\", hue=\"gender\").set_title(\"Age distribution of each assigned gender\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tohtTfaqOAjy",
    "mVWYYaksznxN",
    "JuE3zLU-Fwmj",
    "RgNnybR3sLJ2",
    "MSHebS7RYurv",
    "V5_REunNmANR"
   ],
   "name": "P2-milestone.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "e78a7ef29a5e3028f948eff69c34ba1d8ebd35a887497a02775c6aab840f6bc2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "429.85px",
    "left": "847px",
    "right": "20px",
    "top": "120px",
    "width": "383px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
