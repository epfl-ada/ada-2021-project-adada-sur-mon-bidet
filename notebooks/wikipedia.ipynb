{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia feature engineering\n",
    "\n",
    "The wikipedia part showcases our capabilities to extract features from Wikipedia using QIDs. Such features are gender, political assignation or age. Wikipedia data is quite messy and the heuristics used to extract these features are shown.\n",
    "\n",
    "These features should prove useful to get interesting insights during Milestone 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lucastrg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lucastrg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lucastrg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lucastrg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/home/lucastrg/FLEP/MA1/ADA/ada-2021-project-adada-sur-mon-bidet/notebooks/../helpers/helpers.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../helpers/')\n",
    "sys.path.append('../feature_engineering/')\n",
    "import helpers\n",
    "\n",
    "reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "We will remove all the quotes without a speaker, and we will extract the set of all the speakers and QIDs of the sampled rows.\n",
    "We then fetch a json of each speaker's whole page as well as all its PIDs and RIDs (these 2 IDs are not yet in use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"../data/WE_pref_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers=list(set(df2[\"speaker\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_template=\"https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&titles={}&formatversion=2&rvprop=content&rvslots=*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't work atm, not really useful so might drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia data fetching\n",
    "Fetches all we need to know about a speaker (using their name). Handles one redirection if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint  0\n",
      "Checkpoint  1\n",
      "Checkpoint  2\n",
      "Checkpoint  3\n",
      "Checkpoint  4\n",
      "Checkpoint  5\n",
      "Checkpoint  6\n",
      "Checkpoint  7\n",
      "Checkpoint  8\n",
      "Checkpoint  9\n",
      "Checkpoint  10\n",
      "Checkpoint  11\n",
      "Checkpoint  12\n",
      "Checkpoint  13\n",
      "Checkpoint  14\n",
      "Checkpoint  15\n",
      "Checkpoint  16\n",
      "Checkpoint  17\n",
      "Checkpoint  18\n",
      "Checkpoint  19\n",
      "Checkpoint  20\n",
      "Checkpoint  21\n",
      "Checkpoint  22\n",
      "Checkpoint  23\n",
      "Checkpoint  24\n",
      "Checkpoint  25\n",
      "Checkpoint  26\n",
      "Checkpoint  27\n",
      "Checkpoint  28\n",
      "Checkpoint  29\n"
     ]
    }
   ],
   "source": [
    "invalid_speakers=[]\n",
    "chunks_numbers=30\n",
    "for i,speaker_chunk in enumerate(np.array_split(speakers,chunks_numbers)):\n",
    "\n",
    "   speaker_cache={}\n",
    "   speaker_mapping={}\n",
    "   if i >=30: #Checkpoint\n",
    "      for speaker in speaker_chunk:\n",
    "         try :\n",
    "            with urllib.request.urlopen(request_template.format(urllib.parse.quote(speaker))) as response:\n",
    "               raw_data = json.load(response)[\"query\"][\"pages\"][0]\n",
    "               \n",
    "               if raw_data.get(\"missing\",False):\n",
    "                  invalid_speakers.append(speaker)\n",
    "               else:\n",
    "                  content = raw_data[\"revisions\"][0][\"slots\"][\"main\"][\"content\"]\n",
    "                  if re.search(\"^'''{}''' may refer to\".format(speaker),content): #Drop disambiguation pages\n",
    "                     invalid_speakers.append(speaker)\n",
    "\n",
    "                  else:\n",
    "                     if re.search(\"(^#REDIRECT \\[\\[)([A-Za-z 'À-ÿZİı.-]*)\", content): #Allows to fix most redirecting problems \n",
    "                        speaker_alt = re.search(\"(^#REDIRECT \\[\\[)([A-Za-z 'À-ÿZİı.-]*)\", content).group(2)\n",
    "                        #print(\"Redirect \", speaker ,\"->\",speaker_alt) #Je laisse le print parce qu'il est satisfaisant\n",
    "                        if speaker_alt:\n",
    "                           with urllib.request.urlopen(request_template.format(urllib.parse.quote(speaker_alt))) as response:\n",
    "                              raw_data = json.load(response)[\"query\"][\"pages\"][0]\n",
    "                              if raw_data.get(\"missing\",False):\n",
    "                                 invalid_speakers.append(speaker)\n",
    "                              else:\n",
    "                                 content = raw_data[\"revisions\"][0][\"slots\"][\"main\"][\"content\"]\n",
    "                        else :\n",
    "                           content = \"ERROR\"\n",
    "                     speaker_mapping[speaker]=raw_data[\"title\"]\n",
    "                     speaker_cache[raw_data[\"title\"]]=content\n",
    "                  \n",
    "         except urllib.request.HTTPError :\n",
    "            invalid_speakers.append(speaker)\n",
    "   with open('../speaker_cache/speaker_content_cache.pickle', 'ab') as handle:\n",
    "      pickle.dump(speaker_cache, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "   with open('../speaker_cache/speaker_mapping_cache.pickle', 'ab') as handle:\n",
    "      pickle.dump(speaker_mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      print(\"Checkpoint \", i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb06c098c3216c54642aa07de91d9226ba647e37838e3d6108e31e8e2859f3a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
